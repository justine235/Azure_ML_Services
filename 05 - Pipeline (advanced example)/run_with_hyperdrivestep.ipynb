{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "#!pip install azureml-contrib-fairness\r\n",
    "#!pip install fairlearn==0.4.6\r\n",
    "#!pip install raiwidgets\r\n",
    "#pip install azureml-dataprep[pandas]\r\n",
    "\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.core import *\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core import Workspace, Dataset, Datastore\r\n",
    "import os, shutil\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
    "from azureml.core import Workspace, Dataset\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.core import Workspace,RunConfiguration\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.pipeline.steps import AutoMLStep\r\n",
    "from azureml.train.automl.utilities import get_primary_metrics\r\n",
    "from azureml.data.datapath import DataPath\r\n",
    "from azureml.core.datastore import Datastore\r\n",
    "from azureml.pipeline.core import InputPortBinding\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "from azureml.pipeline.core import PipelineParameter\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.pipeline.core import PipelineData, TrainingOutput\r\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\r\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\r\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\r\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\r\n",
    "from azureml.data import OutputFileDatasetConfig\r\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.train.hyperdrive import *\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import get_run\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from fairlearn.reductions import GridSearch\r\n",
    "from fairlearn.reductions import DemographicParity\r\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\r\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from interpret.ext.blackbox import MimicExplainer\r\n",
    "from interpret.ext.glassbox import LGBMExplainableModel\r\n",
    "from raiwidgets import FairnessDashboard\r\n",
    "from raiwidgets import ExplanationDashboard\r\n",
    "from azureml.pipeline.core import Pipeline\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.pipeline.core import Pipeline, StepSequence\r\n",
    "import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "# Environnement\r\n",
    "\r\n",
    "compute_engine = 'jucalcul1'\r\n",
    "cluster_name = 'jucalcul1'\r\n",
    "dataset_name = 'trainingdataset'\r\n",
    "experiment_name = \"Template_hyperdrive\"\r\n",
    "subscription_id = \"----------\"\r\n",
    "resource_group = \"----------\"\r\n",
    "workspace_name = \"deuxiemeespace\"\r\n",
    "\r\n",
    "# if not exist yet\r\n",
    "#ws = Workspace.create(name='myworkspace',\r\n",
    "#                      subscription_id='<azure-subscription-id>',\r\n",
    "#                      resource_group='myresourcegroup',\r\n",
    "#                      create_resource_group=True,\r\n",
    "#                      location='eastus2')\r\n",
    "\r\n",
    "ws = Workspace.get(subscription_id = subscription_id,\r\n",
    "                   resource_group = resource_group,\r\n",
    "                   name = workspace_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "source": [
    "# Compute \r\n",
    "components_dir = '.'\r\n",
    "os.makedirs(components_dir, exist_ok=True)\r\n",
    "\r\n",
    "\r\n",
    "# Verify that the compute cluster exists\r\n",
    "try:\r\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
    "    print('Found existing cluster : ', cluster_name)\r\n",
    "except ComputeTargetException:\r\n",
    "    # If it doesn't already exist, create it\r\n",
    "    try:\r\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=3)\r\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
    "    except Exception as ex:\r\n",
    "        print(ex, cluster_name)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing cluster :  jucalcul1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "# environment preparation : \r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.core import Environment \r\n",
    "\r\n",
    "aml_run_config = RunConfiguration()\r\n",
    "aml_run_config.target = compute_engine\r\n",
    "\r\n",
    "USE_CURATED_ENV = False\r\n",
    "if USE_CURATED_ENV :\r\n",
    "    curated_environment = Environment.get(workspace=ws, name=\"fairnincluded\") #AzureML-Tutorial\r\n",
    "    aml_run_config.environment = curated_environment\r\n",
    "    \r\n",
    "else:\r\n",
    "    aml_run_config.environment.python.user_managed_dependencies = False\r\n",
    "    \r\n",
    "    # Add some packages relied on by data prep step\r\n",
    "    aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\r\n",
    "        conda_packages=['pandas','scikit-learn'], \r\n",
    "        pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]','pandas','joblib','azureml-core==1.20.0','PyJWT==1.7.1','fairlearn',\r\n",
    "        'azureml-defaults==1.32.0','azureml-train==1.32.0','azureml-contrib-fairness==1.32.0','interpret-community==0.18.1',\r\n",
    "        \"interpret-core==0.2.4\",'azureml-interpret==1.32.0','azureml-datadrift==1.32.0','fairlearn==0.7.0','raiwidgets==0.9.4','facets-overview==1.0.0'], \r\n",
    "        pin_sdk_version=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "# Step 0 environnement\r\n",
    "from azureml.data.data_reference import DataReference\r\n",
    "data_folder = os.path.join(os.getcwd(), 'data/')\r\n",
    "os.makedirs(data_folder, exist_ok=True)\r\n",
    "def_blob_store = Datastore.get(ws, 'saving')\r\n",
    "ws.set_default_datastore('saving')\r\n",
    "\r\n",
    "#sklearn_env = Environment.from_conda_specification(name = 'sklearn_envv2', file_path = './conda_dependencies.yml')\r\n",
    "sklearn_env = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\r\n",
    "\r\n",
    "# Step 0 data ingestion \r\n",
    "raw_ds = Dataset.get_by_name(ws, name=dataset_name, version='latest') # training\r\n",
    "\r\n",
    "# Create a Python environment for the pipeline experiment\r\n",
    "#pipeline_environment = Environment.from_conda_specification(name = 'pipeline-env', file_path = './conda_dependencies.yml')\r\n",
    "#pipeline_environment.python.user_managed_dependencies = False # Let Azure ML manage dependencies\r\n",
    "#pipeline_environment.docker.enabled = True # Use a docker container\r\n",
    "# Create a new runconfig object for the pipeline\r\n",
    "#aml_run_config = RunConfiguration()\r\n",
    "# Assign the environment to the run configuration\r\n",
    "#aml_run_config.environment = pipeline_environment\r\n",
    "\r\n",
    "#------------------------------------------------------------------------------------------------------------------------\r\n",
    "# transformed data intermediary data\r\n",
    "#train_datastore_path = OutputFileDatasetConfig(name=\"train_data\", destination=(artifactstore, TRAIN_DIR)).as_upload()\r\n",
    "#val_datastore_path = OutputFileDatasetConfig(name=\"val_data\", destination=(artifactstore, VAL_DIR)).as_upload()\r\n",
    "#test_datastore_path = OutputFileDatasetConfig(name=\"test_data\", destination=(artifactstore, TEST_DIR)).as_upload()\r\n",
    "\r\n",
    "#train_dataset = train_datastore_path.read_delimited_files().register_on_complete(name=AML_TRAIN_DATASET_NAME,\r\n",
    "#                                                          description='Phishing Training Dataset',\r\n",
    "#                                                          tags={'raw data version': raw_dataset.version,\r\n",
    "#                                                                'pipeline version': PIPELINE_VERSION,\r\n",
    "#                                                                'run version': RUN_VERSION})\r\n",
    "\r\n",
    "#val_dataset = val_datastore_path.read_delimited_files().register_on_complete(name=AML_VAL_DATASET_NAME,\r\n",
    "#                                                      description='Phishing Validation Dataset',\r\n",
    "#                                                      tags={'raw data version': raw_dataset.version,\r\n",
    "#                                                            'pipeline version': PIPELINE_VERSION,\r\n",
    "#                                                            'run version': RUN_VERSION})\r\n",
    "\r\n",
    "#test_dataset = test_datastore_path.read_delimited_files().register_on_complete(name=AML_TEST_DATASET_NAME,\r\n",
    "#                                                        description='Phishing Testing Dataset',\r\n",
    "#                                                        tags={'raw data version': raw_dataset.version,\r\n",
    "#                                                              'pipeline version': PIPELINE_VERSION,\r\n",
    "#                                                              'run version': RUN_VERSION})\r\n",
    "# register clean data preprocess ---- \r\n",
    "#step1_output_ds = fileConfig.register_on_complete(name='processed_data', description = 'files from step1')\r\n",
    "\r\n",
    "#------------------------------------------------------------------------------------------------------------------------\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# specific path for object saving\r\n",
    "fileConfig = OutputFileDatasetConfig(name='file_dataset', destination=(def_blob_store, \"21-09-2021_template/last_structured/file\")).as_mount()\r\n",
    "fileConfig2 = OutputFileDatasetConfig(name='file_dataset2', destination=(def_blob_store, \"21-09-2021_template/last_structured/file\")).as_mount()\r\n",
    "model_path = OutputFileDatasetConfig(name='model', destination=(def_blob_store, \"21-09-2021_template/last_structured/model\")).as_mount()\r\n",
    "model_path2 = OutputFileDatasetConfig(name='model2', destination=(def_blob_store, \"21-09-2021_template/last_structured/model\")).as_mount()\r\n",
    "metrics_path = OutputFileDatasetConfig(name='metrics', destination=(def_blob_store, \"21-09-2021_template/last_structured/metrics\")).as_mount()\r\n",
    "dataset_param = PipelineData('dataset')  # provide output of one step and an input to the one more step further\r\n",
    "datadir_param = PipelineData('datadir', is_directory=True)\r\n",
    "\r\n",
    "\r\n",
    "# Step 1 data preparation\r\n",
    "step1 = PythonScriptStep(name = 'prepare data',\r\n",
    "                         source_directory = 'scripts',\r\n",
    "                         script_name = 'data_prep.py',\r\n",
    "                         compute_target = compute_engine,\r\n",
    "                         runconfig=aml_run_config,\r\n",
    "                         allow_reuse=True,\r\n",
    "                         outputs=[dataset_param, datadir_param],\r\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'), # has_named_input is the method for tabular dataset \r\n",
    "                                      '--dataset', dataset_param, \r\n",
    "                                      '--datadir', datadir_param\r\n",
    "                                      ])\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : training \r\n",
    "train_src = ScriptRunConfig(source_directory = 'scripts',\r\n",
    "                            script = 'model_script2.py',\r\n",
    "                            compute_target = compute_engine,\r\n",
    "                            arguments = [\"--input-dir\", fileConfig, # directory to save all objects\r\n",
    "                                         '--dataset', dataset_param, #get data from previous step\r\n",
    "                                         '--metrics-path', metrics_path,\r\n",
    "                                         '--model-path',model_path  # model path\r\n",
    "                                         ],\r\n",
    "                            environment = sklearn_env)\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : hyperparameter \r\n",
    "param_sampling = RandomParameterSampling( {\r\n",
    "    \"--n_estimators\": choice(15, 50, 100, 200, 300),\r\n",
    "    \"--criterion\": choice(\"gini\", \"entropy\"),\r\n",
    "    \"--max_depth\": choice(2, 8, 16)\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "hd_config = HyperDriveConfig(run_config=train_src, \r\n",
    "                             hyperparameter_sampling=param_sampling,\r\n",
    "                             primary_metric_name='precision', \r\n",
    "                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
    "                             max_total_runs=4, # small nb for testing \r\n",
    "                             max_concurrent_runs=2)\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "hd_step = HyperDriveStep(\r\n",
    "    allow_reuse=True,\r\n",
    "    inputs=[dataset_param, datadir_param],\r\n",
    "    name='hyperparameters',\r\n",
    "    hyperdrive_config=hd_config)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : register best model\r\n",
    "register_model = PythonScriptStep(name = 'Model Registration & Evaluation',\r\n",
    "                                  script_name = 'scripts/register_model2.py',\r\n",
    "                                  compute_target = compute_engine,\r\n",
    "                                  runconfig = aml_run_config,\r\n",
    "                                  allow_reuse = True,\r\n",
    "                                  arguments=[\r\n",
    "                                          '--data_datastore_path', fileConfig2,\r\n",
    "                                          '--model_path', model_path2\r\n",
    "                                          ],\r\n",
    "                                  )\r\n",
    "\r\n",
    "\r\n",
    "# step 4 : publish pipeline\r\n",
    "pipeline_training = [step1, hd_step, register_model]\r\n",
    "pipeline = Pipeline(workspace=ws, steps =StepSequence(steps=pipeline_training))\r\n",
    "pipeline_run = pipeline.submit(experiment_name, regenerate_outputs=True)\r\n",
    "\r\n",
    "published_pipeline1 = pipeline.publish(\r\n",
    "                        name=\"Template_Pipeline_Notebook\",\r\n",
    "                        description=\"Published Pipeline Description\",\r\n",
    "                        version=\"1.0\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 5 : schedule pipeline run every day\r\n",
    "#recurrence = ScheduleRecurrence(frequency='Day', interval=1)\r\n",
    "#recurring_schedule = Schedule.create(ws, name='DailySchedule', \r\n",
    "#                            description='Once a day',\r\n",
    "#                            pipeline_id=published_pipeline1.id, \r\n",
    "#                            experiment_name= experiment_name, \r\n",
    "#                            recurrence=recurrence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step prepare data [be6913af][c8f9d903-cec4-469a-89b0-ebd7f1518d6d], (This step will run and generate new outputs)\n",
      "Created step hyperparameters [7c4b9fd4][9fb51f9d-a2a5-4ef1-b1dc-207563cc6966], (This step will run and generate new outputs)\n",
      "Created step Model Registration & Evaluation [636876c0][104bc43d-b209-42a2-845f-108d2a4d431e], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun f290d8f7-7bdb-4bee-9302-da743cbb1e05\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/f290d8f7-7bdb-4bee-9302-da743cbb1e05?wsid=/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourcegroups/ressourcesecu/workspaces/deuxiemeespace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "# Model Output Arguments\r\n",
    "run = Run.get_context()\r\n",
    "\r\n",
    "\r\n",
    "for hyperdrive_run in pipeline_run.get_children(tags={\"azureml.pipelineComponent\" : \"masterhyperdrivecloud\"}):\r\n",
    "    for train_run in hyperdrive_run.get_children():\r\n",
    "        TRAIN_RUN_ID = train_run.get_details()[\"runId\"]\r\n",
    "        #BEST_CHILD_RUN_ID = train_run.get_details()['properties']\r\n",
    "        #print(BEST_CHILD_RUN_ID)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "source": [
    "pipeline_run.get_children(tags={\"azureml.pipelineComponent\" : \"masterhyperdrivecloud\"})"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<generator object Run._rehydrate_runs at 0x0000020208728C80>"
      ]
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "source": [
    "for child_run in pipeline_run.get_children():\r\n",
    "    listid = child_run.id\r\n",
    "    print('id parent', listid)\r\n",
    "    for  child_run2 in child_run.get_children():\r\n",
    "        print('id pipeline step',child_run2)\r\n",
    "        for  child_run3 in child_run2.get_children():\r\n",
    "            listid = child_run3.id\r\n",
    "            summaryid.append(listid)\r\n",
    "            #list_metric = child_run3.get_metrics()['Test Precison']\r\n",
    "            #summarymetric.append(list_metric)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "pipeline_run.get_details()['properties']\r\n",
    "summaryid = [0]\r\n",
    "summarymetric = [0]\r\n",
    "\r\n",
    "for child_run in pipeline_run.get_children():\r\n",
    "    listid = child_run.id\r\n",
    "    print('id parent', listid)\r\n",
    "    for  child_run2 in child_run.get_children():\r\n",
    "        print('id pipeline step',child_run2)\r\n",
    "        for  child_run3 in child_run2.get_children():\r\n",
    "            listid = child_run3.id\r\n",
    "            summaryid.append(listid)\r\n",
    "            list_metric = child_run3.get_metrics()['Test Precison']\r\n",
    "            summarymetric.append(list_metric)\r\n",
    "\r\n",
    "print(summarymetric)\r\n",
    "print(summaryid)\r\n",
    "maxi = summarymetric.index(max(summarymetric))\r\n",
    "maxi_id = summaryid[maxi]\r\n",
    "print('best model id : ', maxi_id)\r\n",
    "import pickle\r\n",
    "\r\n",
    "run_context = Run.get_context()\r\n",
    "print('start register model')\r\n",
    "for child_run3 in child_run2.get_children():\r\n",
    "    if child_run3.id == maxi_id:\r\n",
    "        #child_run3.register_model(model_name='rf', model_path='outputs/model/saved_model.pkl')\r\n",
    "        print(\"ok it's saved\")\r\n",
    "        #model = child_run3.upload_file('outputs/model/saved_model.pkl','outputs/model/saved_model.pkl')\r\n",
    "        print(model)\r\n",
    "       # model = pickle.load(open('outputs/model/saved_model.pkl', 'rb'))\r\n",
    "        metrics = child_run3.get_file_names()\r\n",
    "        print(metrics)\r\n",
    "        #df = pd.DataFrame(list(metrics.items()),columns = ['metrics','value']) \r\n",
    "        print(df)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0]\n",
      "[0]\n",
      "best model id :  0\n",
      "start register model\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'child_run2' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-c48cb1d97988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mrun_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'start register model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mchild_run3\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchild_run2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_children\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchild_run3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmaxi_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m#child_run3.register_model(model_name='rf', model_path='outputs/model/saved_model.pkl')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'child_run2' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for child_run3 in child_run2.get_children():\r\n",
    "    if child_run3.id == maxi_id:\r\n",
    "        #child_run3.register_model(model_name='rf_tuning', model_path='outputs_here/model.pkl')\r\n",
    "        #print(\"ok it's saved\")\r\n",
    "        #print(child_run3.get_details())"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f4d2fc3f9599656570a986cacd0f8fb633e1f5eb6c0d7edd902095b821fab887"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}