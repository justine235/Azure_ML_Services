{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#!pip install azureml-contrib-fairness\r\n",
    "#!pip install fairlearn==0.4.6\r\n",
    "#!pip install raiwidgets\r\n",
    "#pip install azureml-dataprep[pandas]\r\n",
    "\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.core import *\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core import Workspace, Dataset, Datastore\r\n",
    "import os, shutil\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
    "from azureml.core import Workspace, Dataset\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.core import Workspace,RunConfiguration\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.pipeline.steps import AutoMLStep\r\n",
    "from azureml.train.automl.utilities import get_primary_metrics\r\n",
    "from azureml.data.datapath import DataPath\r\n",
    "from azureml.core.datastore import Datastore\r\n",
    "from azureml.pipeline.core import InputPortBinding\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "from azureml.pipeline.core import PipelineParameter\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.pipeline.core import PipelineData, TrainingOutput\r\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\r\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\r\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\r\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\r\n",
    "from azureml.data import OutputFileDatasetConfig\r\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.train.hyperdrive import *\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import get_run\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from fairlearn.reductions import GridSearch\r\n",
    "from fairlearn.reductions import DemographicParity\r\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\r\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "from sklearn.impute import SimpleImputer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from interpret.ext.blackbox import MimicExplainer\r\n",
    "from interpret.ext.glassbox import LGBMExplainableModel\r\n",
    "from raiwidgets import FairnessDashboard\r\n",
    "from raiwidgets import ExplanationDashboard\r\n",
    "from azureml.pipeline.core import Pipeline\r\n",
    "from azureml.core import Environment\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Environnement\r\n",
    "subscription_id = 'a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b'\r\n",
    "resource_group = '-----------'\r\n",
    "workspace_name = '---------'\r\n",
    "compute_engine = 'jucalcul1'\r\n",
    "cluster_name = 'jucalcul1'\r\n",
    "dataset_name = 'trainingdataset'\r\n",
    "experiment_name = \"25082021\"\r\n",
    "\r\n",
    "# if not exist yet\r\n",
    "#ws = Workspace.create(name='myworkspace',\r\n",
    "#                      subscription_id='<azure-subscription-id>',\r\n",
    "#                      resource_group='myresourcegroup',\r\n",
    "#                      create_resource_group=True,\r\n",
    "#                      location='eastus2')\r\n",
    "\r\n",
    "ws = Workspace.get(subscription_id = \"--------------\",\r\n",
    "                   resource_group = \"------------------\",\r\n",
    "                   name = \"-----------\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Compute \r\n",
    "components_dir = '.'\r\n",
    "os.makedirs(components_dir, exist_ok=True)\r\n",
    "\r\n",
    "\r\n",
    "# Verify that the compute cluster exists\r\n",
    "try:\r\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
    "    print('Found existing cluster : ', cluster_name)\r\n",
    "except ComputeTargetException:\r\n",
    "    # If it doesn't already exist, create it\r\n",
    "    try:\r\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=3)\r\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
    "    except Exception as ex:\r\n",
    "        print(ex, cluster_name)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing cluster :  jucalcul1\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# environment preparation : \r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.core import Environment \r\n",
    "\r\n",
    "aml_run_config = RunConfiguration()\r\n",
    "aml_run_config.target = compute_engine\r\n",
    "\r\n",
    "USE_CURATED_ENV = True\r\n",
    "if USE_CURATED_ENV :\r\n",
    "    curated_environment = Environment.get(workspace=ws, name=\"AzureML\")\r\n",
    "    aml_run_config.environment = curated_environment\r\n",
    "else:\r\n",
    "    aml_run_config.environment.python.user_managed_dependencies = False\r\n",
    "    \r\n",
    "    # Add some packages relied on by data prep step\r\n",
    "    aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\r\n",
    "        conda_packages=['pandas','scikit-learn'], \r\n",
    "        pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]','pandas'], \r\n",
    "        pin_sdk_version=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Step 0 environnement\r\n",
    "data_folder = os.path.join(os.getcwd(), 'data/')\r\n",
    "os.makedirs(data_folder, exist_ok=True)\r\n",
    "sklearn_env = Environment.from_conda_specification(name = 'sklearn_envv2', file_path = './conda_dependencies.yml')\r\n",
    "\r\n",
    "# Step 0 data ingestion\r\n",
    "raw_ds = Dataset.get_by_name(ws, name=dataset_name, version='latest') \r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "\r\n",
    "# allow to use output1 in input2\r\n",
    "fileConfig = OutputFileDatasetConfig(name='file_dataset')\r\n",
    "\r\n",
    "# Step 1 data preparation\r\n",
    "step1 = PythonScriptStep(name = 'prepare data',\r\n",
    "                         source_directory = 'scripts',\r\n",
    "                         script_name = 'data_prep.py',\r\n",
    "                         compute_target = compute_engine,\r\n",
    "                         runconfig=aml_run_config,\r\n",
    "                         allow_reuse=True,\r\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'), # has_named_input is the method for tabular dataset \r\n",
    "                                      '--output-dir', fileConfig])\r\n",
    "\r\n",
    "# Step 2 : training \r\n",
    "train_src = ScriptRunConfig(source_directory = 'scripts',\r\n",
    "                            script = 'model_script2.py',\r\n",
    "                            compute_target = compute_engine,\r\n",
    "                            arguments = [\"--input-dir\", fileConfig.as_input()],\r\n",
    "                            environment = sklearn_env)\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : hyperparameter \r\n",
    "param_sampling = RandomParameterSampling( {\r\n",
    "    \"--n_estimators\": choice(15, 50, 100, 200, 300),\r\n",
    "    \"--criterion\": choice(\"gini\", \"entropy\"),\r\n",
    "    \"--max_depth\": choice(2, 8, 16)\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "hd_config = HyperDriveConfig(run_config=train_src, \r\n",
    "                             hyperparameter_sampling=param_sampling,\r\n",
    "                             primary_metric_name='precision', \r\n",
    "                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
    "                             max_total_runs=1, # 1 for testing \r\n",
    "                             max_concurrent_runs=4)\r\n",
    "\r\n",
    "\r\n",
    "saved_model = PipelineData(name='saved_model',\r\n",
    "                           datastore=datastore,\r\n",
    "                           pipeline_output_name='model_output',\r\n",
    "                           training_output=TrainingOutput(type = \"Model\", metric=\"Precision\", model_file=\"outputs/model/save_model.pkl\"))\r\n",
    "\r\n",
    "metrics_data = PipelineData(name='metrics_data', \r\n",
    "                            datastore=datastore,\r\n",
    "                            pipeline_output_name='metrics_output',\r\n",
    "                            training_output=TrainingOutput(type='Metrics'))\r\n",
    "\r\n",
    "                                \r\n",
    " \r\n",
    "hd_step = HyperDriveStep(\r\n",
    "    allow_reuse=True,\r\n",
    "    name='hyperparameters',\r\n",
    "    outputs = [metrics_data, saved_model],\r\n",
    "    hyperdrive_config=hd_config,\r\n",
    "                             )\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : register best model\r\n",
    "# best model is retrieve directly from PipelineData saved model\r\n",
    "register_model = PythonScriptStep(name = 'Model Registration',\r\n",
    "                                  script_name = 'scripts/register_model.py',\r\n",
    "                                  inputs = [metrics_data, saved_model],\r\n",
    "                                  compute_target = compute_engine,\r\n",
    "                                  allow_reuse=True,\r\n",
    "                                  arguments = [\"--saved-model\", saved_model,\r\n",
    "                                                \"--model-file\", \"model.pkl\",\r\n",
    "                                                \"--model-description\",\"pipeline with hyperameters\",\r\n",
    "                                                \"--model-name\",\"best_model_2508\"]\r\n",
    "                                  )\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : publish pipeline\r\n",
    "pipeline_training = [step1, hd_step, register_model]\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[pipeline_training])\r\n",
    "pipeline_run = pipeline.submit(experiment_name, regenerate_outputs=True)\r\n",
    "\r\n",
    "#published_pipeline1 = pipeline.publish(\r\n",
    "#                        name=\"Template_Pipeline_Notebook\",\r\n",
    "#                        description=\"Published Pipeline Description\",\r\n",
    "#                        version=\"1.0\")\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : schedule pipeline run every day\r\n",
    "#recurrence = ScheduleRecurrence(frequency='Day', interval=1)\r\n",
    "#recurring_schedule = Schedule.create(ws, name='DailySchedule', \r\n",
    "#                            description='Once a day',\r\n",
    "#                            pipeline_id=published_pipeline1.id, \r\n",
    "#                            experiment_name= experiment_name, \r\n",
    "#                            recurrence=recurrence)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No Python version provided, defaulting to \"3.6.2\"\n",
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step prepare data [045bfae9][c4539b2b-4fe0-4c00-a14d-7d4f556a6088], (This step will run and generate new outputs)Created step hyperparameters [8bc0df80][b71edf20-6a18-49c9-a42b-3ff77424725d], (This step will run and generate new outputs)\n",
      "\n",
      "Created step Model Registration [7eb7e2cd][bdf0d52e-da78-4f9d-9a03-d6770386bb06], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 4a31bd0a-4c28-4324-bef6-a1b12296f79d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/4a31bd0a-4c28-4324-bef6-a1b12296f79d?wsid=/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourcegroups/ressourcesecu/workspaces/deuxiemeespace&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f4d2fc3f9599656570a986cacd0f8fb633e1f5eb6c0d7edd902095b821fab887"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}