{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "UC : Full pipeline to be launch every months\r\n",
    "Data ingestion from a business app in order to feed the Blob Storage\r\n",
    "\r\n",
    "\r\n",
    "#Add Environnement Creation\r\n",
    "#Add Compute engine on & off \r\n",
    "\r\n",
    "# Pipeline 0 : reading (data_ingestion) connected with Blob Storage\r\n",
    "# Pipeline 1 : data processing \r\n",
    "# Pipeline 2 : hyperparameters + best model  \r\n",
    "# Pipeline 3 : register best model + save pipeline + schedule every months\r\n",
    "# Final steps : monitoring\r\n",
    "\r\n",
    "- integrate metrics - ok \r\n",
    "- integrate fairness \r\n",
    "- integrate graph explainability\r\n",
    "- inference controle (best model + drift detector) \r\n",
    "- add a graph (learning curve)\r\n",
    "\r\n",
    "# + feed new data from PowerApps  \r\n",
    "# Azure DevOps ? AzureMlOps ?\r\n",
    "# cout carbonne associée à l'experimentation \r\n",
    "# AZ keyvaults for subscription key\r\n",
    "\r\n",
    "\r\n",
    "#- model register in the model library\r\n",
    "#- display all metrics in one tab from all childs and not one by one\r\n",
    "#- update blob storage from data store, is it automatic or not ???????\r\n",
    "#- UI display fairness dashboard in preview screen\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%writefile conda_dependencies.yml\r\n",
    "\r\n",
    "dependencies:\r\n",
    "- python=3.6.2\r\n",
    "- pip:\r\n",
    "  - azureml-defaults\r\n",
    "  - keras\r\n",
    "  - tensorflow<=2.4.*\r\n",
    "  - numpy\r\n",
    "  - scikit-learn\r\n",
    "  - pandas\r\n",
    "  - matplotlib\r\n",
    "  - raiwidgets\r\n",
    "  - fairlearn==0.4.6\r\n",
    "  - azureml-contrib-fairness"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install azureml-contrib-fairness\r\n",
    "#!pip install fairlearn==0.4.6\r\n",
    "#!pip install raiwidgets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "from azureml.core import Dataset\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
    "from azureml.core import Workspace, Dataset\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.core import Workspace,RunConfiguration\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.pipeline.steps import AutoMLStep\r\n",
    "from azureml.train.automl.utilities import get_primary_metrics\r\n",
    "from azureml.data.datapath import DataPath\r\n",
    "from azureml.core.datastore import Datastore\r\n",
    "from azureml.pipeline.core import InputPortBinding\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "from azureml.pipeline.core import PipelineParameter\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.pipeline.core import PipelineData, TrainingOutput\r\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\r\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\r\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\r\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\r\n",
    "from azureml.data import OutputFileDatasetConfig\r\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.train.hyperdrive import *\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import get_run\r\n",
    "\r\n",
    "\r\n",
    "subscription_id = 'a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b'\r\n",
    "resource_group = 'learning'\r\n",
    "workspace_name = 'training_MLservices'\r\n",
    "compute_engine = 'jcharley3'\r\n",
    "dataset_name = 'trainingdataset'\r\n",
    "\r\n",
    "# Step 0 environnement\r\n",
    "myenv = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')\r\n",
    "data_folder = os.path.join(os.getcwd(), 'data/')\r\n",
    "os.makedirs(data_folder, exist_ok=True)\r\n",
    "ws = Workspace.from_config()\r\n",
    "ws.get_details()\r\n",
    "\r\n",
    "# Step 0 data ingestion\r\n",
    "raw_ds = Dataset.get_by_name(ws, name=dataset_name, version='latest')\r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "# allow to use output1 in input2\r\n",
    "fileConfig = OutputFileDatasetConfig(name='file_dataset')\r\n",
    "\r\n",
    "# Step 1 data preparation\r\n",
    "step1 = PythonScriptStep(name = 'prepare data',\r\n",
    "                         source_directory = 'scripts',\r\n",
    "                         script_name = 'data_prep.py',\r\n",
    "                         compute_target = compute_engine,\r\n",
    "                         allow_reuse = True,\r\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),\r\n",
    "                                      '--output-dir', fileConfig])\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : training \r\n",
    "train_src = ScriptRunConfig(source_directory='scripts',\r\n",
    "                            script='model_script2.py',\r\n",
    "                            compute_target= compute_engine,\r\n",
    "                            arguments = [\"--input-dir\", fileConfig.as_input()],\r\n",
    "                            environment=myenv)\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : hyperparameter \r\n",
    "param_sampling = RandomParameterSampling( {\r\n",
    "    \"--n_estimators\": choice(15, 50, 100, 200, 300),\r\n",
    "    \"--criterion\": choice(\"gini\", \"entropy\"),\r\n",
    "    \"--max_depth\": choice(2, 8, 16)\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "hd_config = HyperDriveConfig(run_config=train_src, \r\n",
    "                             hyperparameter_sampling=param_sampling,\r\n",
    "                             primary_metric_name='precision', \r\n",
    "                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
    "                             max_total_runs=3, # 1 for testing \r\n",
    "                             max_concurrent_runs=4)\r\n",
    "\r\n",
    "\r\n",
    "saved_model = PipelineData(name='saved_model',\r\n",
    "                            datastore=datastore,\r\n",
    "                            pipeline_output_name='model_output',\r\n",
    "                            #training_output=TrainingOutput(type = \"Model\",model_file=\"model/save_model.pkl\"))\r\n",
    "                            training_output=TrainingOutput(type = \"Model\", metric=\"Precision\"))\r\n",
    "\r\n",
    "metrics_data = PipelineData(name='metrics_data', \r\n",
    "                             datastore=datastore,\r\n",
    "                             pipeline_output_name='metrics_output',\r\n",
    "                            training_output=TrainingOutput(type='Metrics'))\r\n",
    "\r\n",
    "                                \r\n",
    " \r\n",
    "hd_step = HyperDriveStep(\r\n",
    "    allow_reuse=True,\r\n",
    "    name='hyperparameters',\r\n",
    "    outputs = [metrics_data, saved_model],\r\n",
    "    hyperdrive_config=hd_config,\r\n",
    "                             )\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : register best model\r\n",
    "# best model is retrieve directly from PipelineData saved model\r\n",
    "#register_model = PythonScriptStep(name = 'Model Registration',\r\n",
    "#                                  script_name = 'scripts/register_model.py',\r\n",
    "#                                  arguments = [\"--saved-model\", saved_model],\r\n",
    "#                                  inputs = [saved_model],\r\n",
    "#                                  compute_target = compute_engine\r\n",
    "#                                  )\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : publish pipeline\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[step1, hd_step], description=\"test-pipeline_3\")\r\n",
    "pipeline_run = pipeline.submit(\"end-to-end-demo\", regenerate_outputs=True)\r\n",
    "\r\n",
    "published_pipeline1 = pipeline.publish(\r\n",
    "                        name=\"Template_Pipeline_Notebook\",\r\n",
    "                        description=\"Published Pipeline Description\",\r\n",
    "                        version=\"1.0\")\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : schedule pipeline run every day\r\n",
    "recurrence = ScheduleRecurrence(frequency='Day', interval=1)\r\n",
    "recurring_schedule = Schedule.create(ws, name='DailySchedule', \r\n",
    "                            description='Once a day',\r\n",
    "                            pipeline_id=published_pipeline1.id, \r\n",
    "                            experiment_name='Schedule_endtoend_demo_Pipelines', \r\n",
    "                            recurrence=recurrence)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step prepare data [2459e553][b3d8f5e7-e5c5-4cae-9691-46d5ff1039f1], (This step will run and generate new outputs)\n",
      "Created step hyperparameters [dc929dc0][8919451f-cf4c-44bb-884c-054267954178], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 1e124bcb-0b05-4d45-812c-2bd0da1d8b87\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/1e124bcb-0b05-4d45-812c-2bd0da1d8b87?wsid=/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourcegroups/learning/workspaces/training_MLservices&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "# Retrieve best model from Pipeline Run\r\n",
    "import pickle\r\n",
    "\r\n",
    "best_model_output = pipeline_run.get_pipeline_output(\"model_output\")\r\n",
    "num_file_downloaded = best_model_output.download('.', show_progress=True)\r\n",
    "best_model_output\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ErrorResponseException",
     "evalue": "Unknown error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mErrorResponseException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-165-eb816f86aae6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mbest_model_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pipeline_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model_output\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mnum_file_downloaded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_model_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mbest_model_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\azureml\\pipeline\\core\\run.py\u001b[0m in \u001b[0;36mget_pipeline_output\u001b[1;34m(self, pipeline_output_name)\u001b[0m\n\u001b[0;32m    253\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mazureml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPortDataReference\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \"\"\"\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pipeline_run_provider\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pipeline_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline_output_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait_for_completion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_seconds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\azureml\\pipeline\\core\\_aeva_provider.py\u001b[0m in \u001b[0;36mget_pipeline_output\u001b[1;34m(self, context, pipeline_run_id, pipeline_output_name)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpipeline_output_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m         \"\"\"\n\u001b[1;32m-> 1167\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_service_caller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pipeline_run_output_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline_run_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline_output_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m         data_reference = _AevaPortDataReferenceProvider.get_data_reference_from_output(self._workspace,\n\u001b[0;32m   1169\u001b[0m                                                                                        \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\azureml\\pipeline\\core\\_restclients\\aeva\\service_caller.py\u001b[0m in \u001b[0;36mget_pipeline_run_output_async\u001b[1;34m(self, pipeline_run_id, pipeline_run_output_name)\u001b[0m\n\u001b[0;32m    320\u001b[0m          \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mErrorResponseException\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \"\"\"\n\u001b[1;32m--> 322\u001b[1;33m         result = self._caller.get_pipeline_run_output_async(\n\u001b[0m\u001b[0;32m    323\u001b[0m             \u001b[0msubscription_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_subscription_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_group_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_group_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mworkspace_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workspace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline_run_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline_run_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\azureml\\pipeline\\core\\_restclients\\aeva\\aml_pipelines_api10.py\u001b[0m in \u001b[0;36mget_pipeline_run_output_async\u001b[1;34m(self, subscription_id, resource_group_name, workspace_name, pipeline_run_id, pipeline_run_output_name, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[0;32m   2440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2441\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2442\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mErrorResponseException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2444\u001b[0m         \u001b[0mdeserialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mErrorResponseException\u001b[0m: Unknown error"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(best_model_output._path_on_datastore, \"rb\" ) as f:\r\n",
    "    best_model = pickle.load(f)\r\n",
    "best_model\r\n",
    "\r\n",
    "best_model.steps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retrieve all metrics from Child\r\n",
    "metrics_output = pipeline_run.get_pipeline_output('metrics_output')\r\n",
    "num_file_downloaded = metrics_output.download('.', show_progress=True)\r\n",
    "\r\n",
    "import json\r\n",
    "with open(metrics_output._path_on_datastore) as f:\r\n",
    "    metrics_output_result = f.read()\r\n",
    "    \r\n",
    "deserialized_metrics_output = json.loads(metrics_output_result)\r\n",
    "df = pd.DataFrame(deserialized_metrics_output)\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the HyperDriveStep of the pipeline by name (make sure only 1 exists)\r\n",
    "from azureml.pipeline.core import PipelineRun, StepRun, PortDataReference\r\n",
    "\r\n",
    "pipeline_run = PipelineRun(\"end-to-end-demo\",\"9cc33344-0383-4750-8aef-9e07a0942760\")\r\n",
    "step_run = pipeline_run.find_step_run(\"hd_step\")[0]\r\n",
    "\r\n",
    "# Get RunID for best run (we're lazy)\r\n",
    "best_run_id = hd_step_run.get_best_run_by_primary_metric().id\r\n",
    "\r\n",
    "# Get all hyperparameters that where tried\r\n",
    "hyperparameters = hd_step_run.get_hyperparameters()\r\n",
    "\r\n",
    "# Get all metrics for the runs\r\n",
    "metrics = hd_step_run.get_metrics()\r\n",
    "\r\n",
    "# Iterate through all runs and print metrics + hyperparameters\r\n",
    "for run_id, hp in hyperparameters.items():\r\n",
    "    print(run_id, \"===========\")\r\n",
    "    print(\"Hyperparameters:\\n\", hp)\r\n",
    "    print(\"Metrics:\\n\", metrics[run_id])\r\n",
    " \r\n",
    "# Just for the best run\r\n",
    "print(\"BEST RUN:\", best_run_id)\r\n",
    "print(\"Hyperparameters for best run:\\n\", hyperparameters[best_run_id])\r\n",
    "print(\"Metrics of best run:\\n\", metrics[best_run_id])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Step 2 bis (choice 2 or 2 bis) Run AutomML\r\n",
    "automl_settings = {\r\n",
    "    \"iteration_timeout_minutes\" : 10,\r\n",
    "    \"iterations\" : 4,\r\n",
    "    \"experiment_timeout_hours\" : 0.10,\r\n",
    "    \"primary_metric\" : 'Precision'\r\n",
    "}\r\n",
    "\r\n",
    "aml_run_config = RunConfiguration()\r\n",
    "automl_config = AutoMLConfig(task = 'classification',\r\n",
    "                             path = '.',\r\n",
    "                             debug_log = 'automated_ml_errors.log',\r\n",
    "                             compute_target = 'jcharley2',\r\n",
    "                             run_configuration = aml_run_config,\r\n",
    "                             featurization = 'auto',\r\n",
    "                             training_data = [prepared_ds.read_delimited_files().as_input(name='prepared_ds')],\r\n",
    "                             label_column_name = 'EmployeeTargeted',\r\n",
    "                             **automl_settings)\r\n",
    "                             \r\n",
    "# add to the pipeline\r\n",
    "step2_bis = AutoMLStep(name='AutoML',\r\n",
    "                automl_config=automl_config,\r\n",
    "                passthru_automl_config=False,\r\n",
    "                #outputs=[metrics_data,model_data],\r\n",
    "                enable_default_model_output=False,\r\n",
    "                enable_default_metrics_output=False,\r\n",
    "                allow_reuse=True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f4d2fc3f9599656570a986cacd0f8fb633e1f5eb6c0d7edd902095b821fab887"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}