{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "UC : Full pipeline to be launch every months\r\n",
    "Data ingestion from a business app in order to feed the Blob Storage\r\n",
    "\r\n",
    "##### 1 Script for Environnement \r\n",
    "# Environnement Creation\r\n",
    "# Add Compute engine on & off \r\n",
    "# Add a way to Update conda_dependencies\r\n",
    "\r\n",
    "PIPELINE\r\n",
    "# Step 0 : reading (data_ingestion) connected with Blob Storage\r\n",
    "# Step 1 : data processing \r\n",
    "# Step 2 : hyperparameters + best model  OR automML => two possibilities or run both pipeline on parallel\r\n",
    "# Step 3 : register best model + save pipeline + schedule every months\r\n",
    "# Final steps : monitoring :\r\n",
    "\r\n",
    "- integrate metrics - ok \r\n",
    "- integrate fairness \r\n",
    "- integrate graph explainability\r\n",
    "- inference controle (best model + drift detector) \r\n",
    "- add a graph (learning curve)\r\n",
    "\r\n",
    "\r\n",
    "# Azure DevOps ? AzureMlOps ?\r\n",
    "# cout carbonne associée à l'experimentation - framework dédié\r\n",
    "# AZ keyvaults for subscription key\r\n",
    "\r\n",
    "\r\n",
    "# other point to integrate\r\n",
    " \r\n",
    "# include federated learn ?\r\n",
    "# advisor footprint avant le lancement ?\r\n",
    "# features stores ?\r\n",
    "# prunning / quantisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "%%writefile conda_dependencies.yml\r\n",
    "\r\n",
    "dependencies:\r\n",
    "- python=3.6.2\r\n",
    "- pip:\r\n",
    "  - azureml-defaults\r\n",
    "  - keras\r\n",
    "  - numpy\r\n",
    "  - scikit-learn\r\n",
    "  - pandas\r\n",
    "  - tensorflow<=2.4\r\n",
    "  - matplotlib\r\n",
    "  - raiwidgets\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting conda_dependencies.yml\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "#!pip install azureml-contrib-fairness\r\n",
    "#!pip install fairlearn==0.4.6\r\n",
    "#!pip install raiwidgets\r\n",
    "#pip install azureml-dataprep[pandas]\r\n",
    "\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.core import *\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "from azureml.core import Workspace, Dataset, Datastore\r\n",
    "import os, shutil\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
    "from azureml.core import Workspace, Dataset\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.core import Workspace,RunConfiguration\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.pipeline.steps import AutoMLStep\r\n",
    "from azureml.train.automl.utilities import get_primary_metrics\r\n",
    "from azureml.data.datapath import DataPath\r\n",
    "from azureml.core.datastore import Datastore\r\n",
    "from azureml.pipeline.core import InputPortBinding\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "from azureml.pipeline.core import PipelineParameter\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.pipeline.core import PipelineData, TrainingOutput\r\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\r\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\r\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\r\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\r\n",
    "from azureml.data import OutputFileDatasetConfig\r\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.train.hyperdrive import *\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import get_run\r\n",
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "# Environnement\r\n",
    "subscription_id = 'a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b'\r\n",
    "resource_group = 'learning'\r\n",
    "workspace_name = 'training_MLservices'\r\n",
    "compute_engine = 'jcharley4'\r\n",
    "cluster_name = 'jcharley4'\r\n",
    "dataset_name = 'trainingdataset'\r\n",
    "experiment_name = \"test1208_v2\"\r\n",
    "\r\n",
    "ws = Workspace.from_config()\r\n",
    "ws.get_details()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': '/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourceGroups/learning/providers/Microsoft.MachineLearningServices/workspaces/training_MLservices',\n",
       " 'name': 'training_MLservices',\n",
       " 'identity': {'principal_id': '6fb9f02e-ba5c-4b16-857c-1652f10215cb',\n",
       "  'tenant_id': '72f988bf-86f1-41af-91ab-2d7cd011db47',\n",
       "  'type': 'SystemAssigned'},\n",
       " 'location': 'eastus2',\n",
       " 'type': 'Microsoft.MachineLearningServices/workspaces',\n",
       " 'tags': {},\n",
       " 'sku': 'Basic',\n",
       " 'workspaceid': '8f029277-68ff-4d81-be8d-ef78d356536a',\n",
       " 'sdkTelemetryAppInsightsKey': 'e1f7b545-6243-4abf-ba76-c5691d2edb62',\n",
       " 'description': '',\n",
       " 'friendlyName': 'training_MLservices',\n",
       " 'creationTime': '2021-07-29T08:47:14.4774914+00:00',\n",
       " 'containerRegistry': '/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourceGroups/learning/providers/Microsoft.ContainerRegistry/registries/8f02927768ff4d81be8def78d356536a',\n",
       " 'keyVault': '/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourcegroups/learning/providers/microsoft.keyvault/vaults/trainingkeyvault571b5289',\n",
       " 'applicationInsights': '/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourcegroups/learning/providers/microsoft.insights/components/traininginsights73a4ffa9',\n",
       " 'storageAccount': '/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourcegroups/learning/providers/microsoft.storage/storageaccounts/trainingstoragef39925970',\n",
       " 'hbiWorkspace': False,\n",
       " 'allowPublicAccessWhenBehindVnet': False,\n",
       " 'provisioningState': 'Succeeded',\n",
       " 'imageBuildCompute': '',\n",
       " 'discoveryUrl': 'https://eastus2.api.azureml.ms/discovery',\n",
       " 'notebookInfo': {'fqdn': 'ml-trainingmlservi-eastus2-8f029277-68ff-4d81-be8d-ef78d356536a.notebooks.azure.net',\n",
       "  'resource_id': 'c399cb6a82e444cca58335ec55be5ff5'}}"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "# Environnement Creation\r\n",
    "\r\n",
    "# Set the directory for the experiment files \r\n",
    "components_dir = '.'\r\n",
    "os.makedirs(components_dir, exist_ok=True)\r\n",
    "\r\n",
    "# Create a directory inside for the register component\r\n",
    "os.makedirs(os.path.join(components_dir, \"register\"), exist_ok=True)\r\n",
    "\r\n",
    "# Verify that the compute cluster exists\r\n",
    "try:\r\n",
    "    # Check for existing compute target\r\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
    "    print('Found existing cluster, use it.')\r\n",
    "except ComputeTargetException:\r\n",
    "    # If it doesn't already exist, create it\r\n",
    "    try:\r\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=3)\r\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
    "    except Exception as ex:\r\n",
    "        print(ex)\r\n",
    "\r\n",
    "# Create a Python environment for the pipeline experiment\r\n",
    "#pipeline_environment = Environment.from_conda_specification(name = 'pipeline-env', \r\n",
    "                                                           #file_path = os.path.join(components_dir, \r\n",
    "                                                           #\"conda_dependencies.yml\"))\r\n",
    "#pipeline_environment.python.user_managed_dependencies = True # Let Azure ML manage dependencies\r\n",
    "#pipeline_environment.docker.enabled = True # Use a docker container\r\n",
    "\r\n",
    "# Create a new runconfig object for the pipeline\r\n",
    "pipeline_run_config = RunConfiguration()\r\n",
    "\r\n",
    "# Use the compute you created above.\r\n",
    "pipeline_run_config.target = pipeline_cluster\r\n",
    "\r\n",
    "# Assign the environment to the run configuration\r\n",
    "#pipeline_run_config.environment = pipeline_environment\r\n",
    "\r\n",
    "myenv = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')\r\n",
    "\r\n",
    "print (\"Pipeline configuration created.\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found existing cluster, use it.\n",
      "Pipeline configuration created.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Step 0 environnement\r\n",
    "data_folder = os.path.join(os.getcwd(), 'data/')\r\n",
    "os.makedirs(data_folder, exist_ok=True)\r\n",
    "\r\n",
    "# Step 0 data ingestion\r\n",
    "raw_ds = Dataset.get_by_name(ws, name=dataset_name, version='latest')\r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "# allow to use output1 in input2\r\n",
    "fileConfig = OutputFileDatasetConfig(name='file_dataset')\r\n",
    "\r\n",
    "packages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\r\n",
    "                                    pip_packages=['azureml-defaults'])\r\n",
    "sklearn_env.python.conda_dependencies = packages\r\n",
    "\r\n",
    "# Step 1 data preparation\r\n",
    "step1 = PythonScriptStep(name = 'prepare data',\r\n",
    "                         source_directory = 'scripts',\r\n",
    "                         script_name = 'data_prep.py',\r\n",
    "                         compute_target = compute_engine,\r\n",
    "                         allow_reuse = True,\r\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),\r\n",
    "                                      '--output-dir', fileConfig])\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : training \r\n",
    "train_src = ScriptRunConfig(source_directory='scripts',\r\n",
    "                            script='model_script2.py',\r\n",
    "                            compute_target= compute_engine,\r\n",
    "                            arguments = [\"--input-dir\", fileConfig.as_input()],\r\n",
    "                            environment=sklearn_env)\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : hyperparameter \r\n",
    "param_sampling = RandomParameterSampling( {\r\n",
    "    \"--n_estimators\": choice(15, 50, 100, 200, 300),\r\n",
    "    \"--criterion\": choice(\"gini\", \"entropy\"),\r\n",
    "    \"--max_depth\": choice(2, 8, 16)\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "hd_config = HyperDriveConfig(run_config=train_src, \r\n",
    "                             hyperparameter_sampling=param_sampling,\r\n",
    "                             primary_metric_name='precision', \r\n",
    "                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
    "                             max_total_runs=3, # 1 for testing \r\n",
    "                             max_concurrent_runs=4)\r\n",
    "\r\n",
    "\r\n",
    "saved_model = PipelineData(name='saved_model',\r\n",
    "                           datastore=datastore,\r\n",
    "                           pipeline_output_name='model_output',\r\n",
    "                           training_output=TrainingOutput(type = \"Model\", metric=\"Precision\", model_file=\"outputs/model/save_model.pkl\"))\r\n",
    "\r\n",
    "metrics_data = PipelineData(name='metrics_data', \r\n",
    "                            datastore=datastore,\r\n",
    "                            pipeline_output_name='metrics_output',\r\n",
    "                            training_output=TrainingOutput(type='Metrics'))\r\n",
    "\r\n",
    "                                \r\n",
    " \r\n",
    "hd_step = HyperDriveStep(\r\n",
    "    allow_reuse=True,\r\n",
    "    name='hyperparameters',\r\n",
    "    outputs = [metrics_data, saved_model],\r\n",
    "    hyperdrive_config=hd_config,\r\n",
    "                             )\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : register best model\r\n",
    "# best model is retrieve directly from PipelineData saved model\r\n",
    "register_model = PythonScriptStep(name = 'Model Registration',\r\n",
    "                                  script_name = 'scripts/register_model.py',\r\n",
    "                                  inputs = [saved_model],\r\n",
    "                                  compute_target = compute_engine,\r\n",
    "                                  allow_reuse=True,\r\n",
    "                                  arguments = [\"--saved-model\", saved_model,\r\n",
    "                                                \"--model-file\", \"model_sab.pkl\",\r\n",
    "                                                \"--model-description\",\"pipeline with hyperameters\",\r\n",
    "                                                \"--model-name\",\"best_model_1208\"]\r\n",
    "                                  )\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : publish pipeline\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[step1, hd_step, register_model], description=\"test-pipeline_3\")\r\n",
    "pipeline_run = pipeline.submit(experiment_name, regenerate_outputs=True)\r\n",
    "\r\n",
    "published_pipeline1 = pipeline.publish(\r\n",
    "                        name=\"Template_Pipeline_Notebook\",\r\n",
    "                        description=\"Published Pipeline Description\",\r\n",
    "                        version=\"1.0\")\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : schedule pipeline run every day\r\n",
    "#recurrence = ScheduleRecurrence(frequency='Day', interval=1)\r\n",
    "#recurring_schedule = Schedule.create(ws, name='DailySchedule', \r\n",
    "#                            description='Once a day',\r\n",
    "#                            pipeline_id=published_pipeline1.id, \r\n",
    "#                            experiment_name= experiment_name, \r\n",
    "#                            recurrence=recurrence)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n",
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step prepare data [80744b84][b60fdcb8-50b9-4c8b-958c-a93162836136], (This step will run and generate new outputs)\n",
      "Created step hyperparameters [d073d36e][16c8928d-23fe-4736-9be3-2019f78d8e77], (This step will run and generate new outputs)\n",
      "Created step Model Registration [739b6af4][8ec1b2de-0d19-448c-a32b-cbbc1cc6b7a0], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 82581b73-2080-47bb-b677-5fb359c0c25d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/82581b73-2080-47bb-b677-5fb359c0c25d?wsid=/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourcegroups/learning/workspaces/training_MLservices&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f4d2fc3f9599656570a986cacd0f8fb633e1f5eb6c0d7edd902095b821fab887"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}