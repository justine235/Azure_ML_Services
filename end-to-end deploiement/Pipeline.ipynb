{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "UC : Full pipeline to be launch every months\r\n",
    "Data ingestion from a business app in order to feed the Blob Storage\r\n",
    "\r\n",
    "# Pipeline 0 : reading (data_ingestion) \r\n",
    "# Pipeline 1 : data processing \r\n",
    "# Pipeline 2 : model + hyperparameters  \r\n",
    "# Pipeline 3 : register best model + save pipeline + schedule every months\r\n",
    "# Final steps : monitoring\r\n",
    "\r\n",
    "- integrate metrics - ok \r\n",
    "- integrate fairness \r\n",
    "- integrate graph explainability\r\n",
    "- inference controle (best model + drift detector) \r\n",
    "- add a graph (learning curve)\r\n",
    "\r\n",
    "# + feed new data from PowerApps  \r\n",
    "# Azure DevOps ? AzureMlOps ?\r\n",
    "# cout carbonne associée à l'experimentation \r\n",
    "# AZ keyvaults for subscription key\r\n",
    "\r\n",
    "\r\n",
    "#- how to know if the model register is the correct one (best model inside hyperdrive step run) \r\n",
    "#- display all metrics in one tab from all childs and not one by one\r\n",
    "#- update blob storage from data store, is it automatic or not ???????\r\n",
    "#- UI display fairness dashboard in preview screen\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "%%writefile conda_dependencies.yml\r\n",
    "\r\n",
    "dependencies:\r\n",
    "- python=3.6.2\r\n",
    "- pip:\r\n",
    "  - azureml-defaults\r\n",
    "  - keras\r\n",
    "  - tensorflow<=2.4.*\r\n",
    "  - numpy\r\n",
    "  - scikit-learn\r\n",
    "  - pandas\r\n",
    "  - matplotlib\r\n",
    "  - raiwidgets\r\n",
    "  - fairlearn==0.4.6\r\n",
    "  - azureml-contrib-fairness"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting conda_dependencies.yml\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "#!pip install azureml-contrib-fairness\r\n",
    "#!pip install fairlearn==0.4.6\r\n",
    "#!pip install raiwidgets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "source": [
    "from azureml.core import Dataset\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
    "from azureml.core import Workspace, Dataset\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.core import Workspace,RunConfiguration\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.pipeline.steps import AutoMLStep\r\n",
    "from azureml.train.automl.utilities import get_primary_metrics\r\n",
    "from azureml.data.datapath import DataPath\r\n",
    "from azureml.core.datastore import Datastore\r\n",
    "from azureml.pipeline.core import InputPortBinding\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "from azureml.pipeline.core import PipelineParameter\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.pipeline.core import PipelineData, TrainingOutput\r\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\r\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\r\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\r\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\r\n",
    "from azureml.data import OutputFileDatasetConfig\r\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.train.hyperdrive import *\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\r\n",
    "\r\n",
    "subscription_id = 'a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b'\r\n",
    "resource_group = 'learning'\r\n",
    "workspace_name = 'training_MLservices'\r\n",
    "compute_engine = 'jcharley3'\r\n",
    "\r\n",
    "# Step 0 environnement\r\n",
    "myenv = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')\r\n",
    "data_folder = os.path.join(os.getcwd(), 'data/')\r\n",
    "os.makedirs(data_folder, exist_ok=True)\r\n",
    "ws = Workspace.from_config()\r\n",
    "ws.get_details()\r\n",
    "\r\n",
    "# Step 0 data ingestion\r\n",
    "raw_ds = Dataset.get_by_name(ws, name='trainingdataset', version=1)\r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "# allow to use output1 in input2\r\n",
    "fileConfig = OutputFileDatasetConfig(name='file_dataset')\r\n",
    "\r\n",
    "# Step 1 data preparation\r\n",
    "step1 = PythonScriptStep(name = 'prepare data',\r\n",
    "                         source_directory = 'scripts',\r\n",
    "                         script_name = 'data_prep.py',\r\n",
    "                         compute_target = compute_engine,\r\n",
    "                         allow_reuse = True,\r\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),\r\n",
    "                                      '--output-dir', fileConfig])\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : training \r\n",
    "train_src = ScriptRunConfig(source_directory='scripts',\r\n",
    "                            script='model_script2.py',\r\n",
    "                            compute_target= compute_engine,\r\n",
    "                            arguments = [\"--input-dir\", fileConfig.as_input()],\r\n",
    "                            environment=myenv)\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : hyperparameter \r\n",
    "param_sampling = RandomParameterSampling( {\r\n",
    "    \"--n_estimators\": choice(15, 50, 100, 200, 300),\r\n",
    "    \"--criterion\": choice(\"gini\", \"entropy\"),\r\n",
    "    \"--max_depth\": choice(2, 8, 16)\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "hd_config = HyperDriveConfig(run_config=train_src, \r\n",
    "                             hyperparameter_sampling=param_sampling,\r\n",
    "                             primary_metric_name='precision', \r\n",
    "                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
    "                             max_total_runs=3, # 1 for testing \r\n",
    "                             max_concurrent_runs=4)\r\n",
    "\r\n",
    "\r\n",
    "saved_model = PipelineData(name='saved_model',\r\n",
    "                            datastore=datastore,\r\n",
    "                            pipeline_output_name='model_output',\r\n",
    "                            #training_output=TrainingOutput(type = \"Model\",model_file=\"model/save_model.pkl\"))\r\n",
    "                            training_output=TrainingOutput(type = \"Model\"))\r\n",
    "\r\n",
    "metrics_data = PipelineData(name='metrics_data', datastore=datastore,\r\n",
    "                                pipeline_output_name='metrics_output',\r\n",
    "                                training_output=TrainingOutput(type='Metrics'))\r\n",
    "\r\n",
    "                                \r\n",
    " \r\n",
    "hd_step = HyperDriveStep(\r\n",
    "    allow_reuse=True,\r\n",
    "    name='hyperparameters',\r\n",
    "    outputs = [metrics_data, saved_model],\r\n",
    "    hyperdrive_config=hd_config,\r\n",
    "                             )\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : register best model\r\n",
    "register_model = PythonScriptStep(name = 'Model Registration',\r\n",
    "                                  script_name = 'scripts/register_model.py',\r\n",
    "                                  arguments = [\"--saved-model\", saved_model],\r\n",
    "                                  inputs = [saved_model],\r\n",
    "                                  compute_target = compute_engine\r\n",
    "                                  )\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : publish pipeline\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[step1, hd_step, register_model], description=\"test-pipeline_3\")\r\n",
    "pipeline_run = pipeline.submit(\"end-to-end-demo\", regenerate_outputs=True)\r\n",
    "\r\n",
    "published_pipeline1 = pipeline.publish(\r\n",
    "                        name=\"Template_Pipeline_Notebook\",\r\n",
    "                        description=\"Published Pipeline Description\",\r\n",
    "                        version=\"1.0\")\r\n",
    "\r\n",
    "# step 3 : schedule pipeline run every day\r\n",
    "recurrence = ScheduleRecurrence(frequency='Day', interval=1)\r\n",
    "recurring_schedule = Schedule.create(ws, name='DailySchedule', \r\n",
    "                            description='Once a day',\r\n",
    "                            pipeline_id=published_pipeline1.id, \r\n",
    "                            experiment_name='Schedule_endtoend_demo_Pipelines', \r\n",
    "                            recurrence=recurrence)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n",
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Created step prepare data [bbef4b2b][d928874b-7080-4650-be55-76394e7c71a3], (This step will run and generate new outputs)\n",
      "Created step hyperparameters [06dc5c02][52072060-f1d9-404c-b561-13dda945f414], (This step will run and generate new outputs)\n",
      "Created step Model Registration [91fb0a36][afe94640-c7bd-46dd-b41c-6a2c68656b6f], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 722a5514-e869-49db-b891-c350af1b85a6\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/722a5514-e869-49db-b891-c350af1b85a6?wsid=/subscriptions/a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b/resourcegroups/learning/workspaces/training_MLservices&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "best_run = hd_config.get_best_run_by_primary_metric()\r\n",
    "best_run_metrics = best_run.get_metrics()\r\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\r\n",
    "\r\n",
    "print('Best Run Id: ', best_run.id)\r\n",
    "print('\\n Accuracy:', best_run_metrics['accuracy'])\r\n",
    "print('\\n learning rate:',parameter_values[3])\r\n",
    "print('\\n keep probability:',parameter_values[5])\r\n",
    "print('\\n batch size:',parameter_values[7])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'HyperDriveConfig' object has no attribute 'get_best_run_by_primary_metric'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-117-b5ec4148e919>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhd_config\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_best_run_by_primary_metric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbest_run_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mparameter_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'runDefinition'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'arguments'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best Run Id: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HyperDriveConfig' object has no attribute 'get_best_run_by_primary_metric'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the HyperDriveStep of the pipeline by name (make sure only 1 exists)\r\n",
    "from azureml.pipeline.core import PipelineRun, StepRun, PortDataReference\r\n",
    "\r\n",
    "pipeline_run = PipelineRun(\"end-to-end-demo\",\"9cc33344-0383-4750-8aef-9e07a0942760\")\r\n",
    "step_run = pipeline_run.find_step_run(\"hd_step\")[0]\r\n",
    "\r\n",
    "# Get RunID for best run (we're lazy)\r\n",
    "best_run_id = hd_step_run.get_best_run_by_primary_metric().id\r\n",
    "\r\n",
    "# Get all hyperparameters that where tried\r\n",
    "hyperparameters = hd_step_run.get_hyperparameters()\r\n",
    "\r\n",
    "# Get all metrics for the runs\r\n",
    "metrics = hd_step_run.get_metrics()\r\n",
    "\r\n",
    "# Iterate through all runs and print metrics + hyperparameters\r\n",
    "for run_id, hp in hyperparameters.items():\r\n",
    "    print(run_id, \"===========\")\r\n",
    "    print(\"Hyperparameters:\\n\", hp)\r\n",
    "    print(\"Metrics:\\n\", metrics[run_id])\r\n",
    " \r\n",
    "# Just for the best run\r\n",
    "print(\"BEST RUN:\", best_run_id)\r\n",
    "print(\"Hyperparameters for best run:\\n\", hyperparameters[best_run_id])\r\n",
    "print(\"Metrics of best run:\\n\", metrics[best_run_id])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Step 2 bis (choice 2 or 2 bis) Run AutomML\r\n",
    "automl_settings = {\r\n",
    "    \"iteration_timeout_minutes\" : 10,\r\n",
    "    \"iterations\" : 4,\r\n",
    "    \"experiment_timeout_hours\" : 0.10,\r\n",
    "    \"primary_metric\" : 'Precision'\r\n",
    "}\r\n",
    "\r\n",
    "aml_run_config = RunConfiguration()\r\n",
    "automl_config = AutoMLConfig(task = 'classification',\r\n",
    "                             path = '.',\r\n",
    "                             debug_log = 'automated_ml_errors.log',\r\n",
    "                             compute_target = 'jcharley2',\r\n",
    "                             run_configuration = aml_run_config,\r\n",
    "                             featurization = 'auto',\r\n",
    "                             training_data = [prepared_ds.read_delimited_files().as_input(name='prepared_ds')],\r\n",
    "                             label_column_name = 'EmployeeTargeted',\r\n",
    "                             **automl_settings)\r\n",
    "                             \r\n",
    "# add to the pipeline\r\n",
    "step2_bis = AutoMLStep(name='AutoML',\r\n",
    "                automl_config=automl_config,\r\n",
    "                passthru_automl_config=False,\r\n",
    "                #outputs=[metrics_data,model_data],\r\n",
    "                enable_default_model_output=False,\r\n",
    "                enable_default_metrics_output=False,\r\n",
    "                allow_reuse=True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f4d2fc3f9599656570a986cacd0f8fb633e1f5eb6c0d7edd902095b821fab887"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}