{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "UC : Full pipeline to be launch every months\r\n",
    "Data ingestion from a business app in order to feed the Blob Storage\r\n",
    "\r\n",
    "##### 1 Script for Environnement \r\n",
    "# Environnement Creation\r\n",
    "# Add Compute engine on & off \r\n",
    "# Add a way to Update conda_dependencies\r\n",
    "\r\n",
    "PIPELINE\r\n",
    "# Step 0 : reading (data_ingestion) connected with Blob Storage\r\n",
    "# Step 1 : data processing \r\n",
    "# Step 2 : hyperparameters + best model  OR automML\r\n",
    "# Step 3 : register best model + save pipeline + schedule every months\r\n",
    "# Final steps : monitoring :\r\n",
    "\r\n",
    "- integrate metrics - ok \r\n",
    "- integrate fairness \r\n",
    "- integrate graph explainability\r\n",
    "- inference controle (best model + drift detector) \r\n",
    "- add a graph (learning curve)\r\n",
    "\r\n",
    "\r\n",
    "# Azure DevOps ? AzureMlOps ?\r\n",
    "# cout carbonne associée à l'experimentation - framework dédié\r\n",
    "# AZ keyvaults for subscription key\r\n",
    "\r\n",
    "\r\n",
    "# other point to integrate\r\n",
    " \r\n",
    "# include federated learn ?\r\n",
    "# advisor footprint avant le lancement ?\r\n",
    "# features stores ?\r\n",
    "# prunning / quantisation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%writefile conda_dependencies.yml\r\n",
    "\r\n",
    "dependencies:\r\n",
    "- python=3.6.2\r\n",
    "- pip:\r\n",
    "  - azureml-defaults\r\n",
    "  - keras\r\n",
    "  - tensorflow<=2.4.*\r\n",
    "  - numpy\r\n",
    "  - scikit-learn\r\n",
    "  - pandas\r\n",
    "  - matplotlib\r\n",
    "  - raiwidgets\r\n",
    "  - fairlearn==0.4.6\r\n",
    "  - azureml-contrib-fairness"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#!pip install azureml-contrib-fairness\r\n",
    "#!pip install fairlearn==0.4.6\r\n",
    "#!pip install raiwidgets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Environnement Creation\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.core.compute_target import ComputeTargetException\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.core.runconfig import RunConfiguration\r\n",
    "import os, shutil\r\n",
    "\r\n",
    "# Set the directory for the experiment files used in Challenge 2\r\n",
    "components_dir = 'safe_driver'\r\n",
    "os.makedirs(components_dir, exist_ok=True)\r\n",
    "\r\n",
    "# Create a directory inside for the register component\r\n",
    "os.makedirs(os.path.join(components_dir, \"register\"), exist_ok=True)\r\n",
    "\r\n",
    "# Verify that the compute cluster exists\r\n",
    "cluster_name = \"train-cluster\"\r\n",
    "\r\n",
    "try:\r\n",
    "    # Check for existing compute target\r\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
    "    print('Found existing cluster, use it.')\r\n",
    "except ComputeTargetException:\r\n",
    "    # If it doesn't already exist, create it\r\n",
    "    try:\r\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\r\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\r\n",
    "    except Exception as ex:\r\n",
    "        print(ex)\r\n",
    "\r\n",
    "# Create a Python environment for the pipeline experiment\r\n",
    "pipeline_environment = Environment.from_conda_specification(name = 'pipeline-env', \r\n",
    "                                                           file_path = os.path.join(components_dir, \r\n",
    "                                                           \"conda_dependencies.yml\"))\r\n",
    "pipeline_environment.python.user_managed_dependencies = False # Let Azure ML manage dependencies\r\n",
    "pipeline_environment.docker.enabled = True # Use a docker container\r\n",
    "\r\n",
    "# Create a new runconfig object for the pipeline\r\n",
    "pipeline_run_config = RunConfiguration()\r\n",
    "\r\n",
    "# Use the compute you created above. \r\n",
    "pipeline_run_config.target = pipeline_cluster\r\n",
    "\r\n",
    "# Assign the environment to the run configuration\r\n",
    "pipeline_run_config.environment = pipeline_environment\r\n",
    "\r\n",
    "print (\"Pipeline configuration created.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Variables\r\n",
    "subscription_id = 'a0f4cddc-a66a-4dcc-9df7-ccbd7f81bf7b'\r\n",
    "resource_group = 'learning'\r\n",
    "workspace_name = 'training_MLservices'\r\n",
    "compute_engine = 'jcharley3'\r\n",
    "dataset_name = 'trainingdataset'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azureml.core import Dataset\r\n",
    "from azureml.pipeline.steps import PythonScriptStep\r\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\r\n",
    "from azureml.core import Workspace, Dataset\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.core import Workspace,RunConfiguration\r\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\r\n",
    "from azureml.train.automl import AutoMLConfig\r\n",
    "from azureml.pipeline.steps import AutoMLStep\r\n",
    "from azureml.train.automl.utilities import get_primary_metrics\r\n",
    "from azureml.data.datapath import DataPath\r\n",
    "from azureml.core.datastore import Datastore\r\n",
    "from azureml.pipeline.core import InputPortBinding\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.data.dataset_consumption_config import DatasetConsumptionConfig\r\n",
    "from azureml.pipeline.core import PipelineParameter\r\n",
    "from azureml.core import Dataset\r\n",
    "from azureml.core import Environment\r\n",
    "from azureml.pipeline.core import PipelineData, TrainingOutput\r\n",
    "from azureml.pipeline.steps import HyperDriveStep, HyperDriveStepRun, PythonScriptStep\r\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\r\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\r\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\r\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\r\n",
    "from azureml.data import OutputFileDatasetConfig\r\n",
    "from azureml.core import Experiment, ScriptRunConfig, Environment\r\n",
    "from azureml.core.conda_dependencies import CondaDependencies\r\n",
    "from azureml.train.hyperdrive import *\r\n",
    "from azureml.widgets import RunDetails\r\n",
    "from azureml.pipeline.core.schedule import ScheduleRecurrence, Schedule\r\n",
    "from azureml.core import Run\r\n",
    "from azureml.core import get_run\r\n",
    "\r\n",
    "\r\n",
    "# Step 0 environnement\r\n",
    "myenv = Environment.from_conda_specification(name = 'sklearn-env', file_path = './conda_dependencies.yml')\r\n",
    "data_folder = os.path.join(os.getcwd(), 'data/')\r\n",
    "os.makedirs(data_folder, exist_ok=True)\r\n",
    "ws = Workspace.from_config()\r\n",
    "ws.get_details()\r\n",
    "\r\n",
    "# Step 0 data ingestion\r\n",
    "raw_ds = Dataset.get_by_name(ws, name=dataset_name, version='latest')\r\n",
    "datastore = ws.get_default_datastore()\r\n",
    "# allow to use output1 in input2\r\n",
    "fileConfig = OutputFileDatasetConfig(name='file_dataset')\r\n",
    "\r\n",
    "# Step 1 data preparation\r\n",
    "step1 = PythonScriptStep(name = 'prepare data',\r\n",
    "                         source_directory = 'scripts',\r\n",
    "                         script_name = 'data_prep.py',\r\n",
    "                         compute_target = compute_engine,\r\n",
    "                         allow_reuse = True,\r\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),\r\n",
    "                                      '--output-dir', fileConfig])\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : training \r\n",
    "train_src = ScriptRunConfig(source_directory='scripts',\r\n",
    "                            script='model_script2.py',\r\n",
    "                            compute_target= compute_engine,\r\n",
    "                            arguments = [\"--input-dir\", fileConfig.as_input()],\r\n",
    "                            environment=myenv)\r\n",
    "\r\n",
    "\r\n",
    "# Step 2 : hyperparameter \r\n",
    "param_sampling = RandomParameterSampling( {\r\n",
    "    \"--n_estimators\": choice(15, 50, 100, 200, 300),\r\n",
    "    \"--criterion\": choice(\"gini\", \"entropy\"),\r\n",
    "    \"--max_depth\": choice(2, 8, 16)\r\n",
    "    }\r\n",
    ")\r\n",
    "\r\n",
    "hd_config = HyperDriveConfig(run_config=train_src, \r\n",
    "                             hyperparameter_sampling=param_sampling,\r\n",
    "                             primary_metric_name='precision', \r\n",
    "                             primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \r\n",
    "                             max_total_runs=3, # 1 for testing \r\n",
    "                             max_concurrent_runs=4)\r\n",
    "\r\n",
    "\r\n",
    "saved_model = PipelineData(name='saved_model',\r\n",
    "                            datastore=datastore,\r\n",
    "                            pipeline_output_name='model_output',\r\n",
    "                            #training_output=TrainingOutput(type = \"Model\",model_file=\"model/save_model.pkl\"))\r\n",
    "                            training_output=TrainingOutput(type = \"Model\", metric=\"Precision\"))\r\n",
    "\r\n",
    "metrics_data = PipelineData(name='metrics_data', \r\n",
    "                             datastore=datastore,\r\n",
    "                             pipeline_output_name='metrics_output',\r\n",
    "                            training_output=TrainingOutput(type='Metrics'))\r\n",
    "\r\n",
    "                                \r\n",
    " \r\n",
    "hd_step = HyperDriveStep(\r\n",
    "    allow_reuse=True,\r\n",
    "    name='hyperparameters',\r\n",
    "    outputs = [metrics_data, saved_model],\r\n",
    "    hyperdrive_config=hd_config,\r\n",
    "                             )\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : register best model\r\n",
    "# best model is retrieve directly from PipelineData saved model\r\n",
    "#register_model = PythonScriptStep(name = 'Model Registration',\r\n",
    "#                                  script_name = 'scripts/register_model.py',\r\n",
    "#                                  arguments = [\"--saved-model\", saved_model],\r\n",
    "#                                  inputs = [saved_model],\r\n",
    "#                                  compute_target = compute_engine\r\n",
    "#                                  )\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : publish pipeline\r\n",
    "pipeline = Pipeline(workspace=ws, steps=[step1, hd_step], description=\"test-pipeline_3\")\r\n",
    "pipeline_run = pipeline.submit(\"end-to-end-demo\", regenerate_outputs=True)\r\n",
    "\r\n",
    "published_pipeline1 = pipeline.publish(\r\n",
    "                        name=\"Template_Pipeline_Notebook\",\r\n",
    "                        description=\"Published Pipeline Description\",\r\n",
    "                        version=\"1.0\")\r\n",
    "\r\n",
    "\r\n",
    "# step 3 : schedule pipeline run every day\r\n",
    "recurrence = ScheduleRecurrence(frequency='Day', interval=1)\r\n",
    "recurring_schedule = Schedule.create(ws, name='DailySchedule', \r\n",
    "                            description='Once a day',\r\n",
    "                            pipeline_id=published_pipeline1.id, \r\n",
    "                            experiment_name='Schedule_endtoend_demo_Pipelines', \r\n",
    "                            recurrence=recurrence)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retrieve best model from Pipeline Run\r\n",
    "import pickle\r\n",
    "\r\n",
    "best_model_output = pipeline_run.get_pipeline_output(\"model_output\")\r\n",
    "num_file_downloaded = best_model_output.download('.', show_progress=True)\r\n",
    "best_model_output\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(best_model_output._path_on_datastore, \"rb\" ) as f:\r\n",
    "    best_model = pickle.load(f)\r\n",
    "best_model\r\n",
    "\r\n",
    "best_model.steps"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retrieve all metrics from Child\r\n",
    "metrics_output = pipeline_run.get_pipeline_output('metrics_output')\r\n",
    "num_file_downloaded = metrics_output.download('.', show_progress=True)\r\n",
    "\r\n",
    "import json\r\n",
    "with open(metrics_output._path_on_datastore) as f:\r\n",
    "    metrics_output_result = f.read()\r\n",
    "    \r\n",
    "deserialized_metrics_output = json.loads(metrics_output_result)\r\n",
    "df = pd.DataFrame(deserialized_metrics_output)\r\n",
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the HyperDriveStep of the pipeline by name (make sure only 1 exists)\r\n",
    "from azureml.pipeline.core import PipelineRun, StepRun, PortDataReference\r\n",
    "\r\n",
    "pipeline_run = PipelineRun(\"end-to-end-demo\",\"9cc33344-0383-4750-8aef-9e07a0942760\")\r\n",
    "step_run = pipeline_run.find_step_run(\"hd_step\")[0]\r\n",
    "\r\n",
    "# Get RunID for best run (we're lazy)\r\n",
    "best_run_id = hd_step_run.get_best_run_by_primary_metric().id\r\n",
    "\r\n",
    "# Get all hyperparameters that where tried\r\n",
    "hyperparameters = hd_step_run.get_hyperparameters()\r\n",
    "\r\n",
    "# Get all metrics for the runs\r\n",
    "metrics = hd_step_run.get_metrics()\r\n",
    "\r\n",
    "# Iterate through all runs and print metrics + hyperparameters\r\n",
    "for run_id, hp in hyperparameters.items():\r\n",
    "    print(run_id, \"===========\")\r\n",
    "    print(\"Hyperparameters:\\n\", hp)\r\n",
    "    print(\"Metrics:\\n\", metrics[run_id])\r\n",
    " \r\n",
    "# Just for the best run\r\n",
    "print(\"BEST RUN:\", best_run_id)\r\n",
    "print(\"Hyperparameters for best run:\\n\", hyperparameters[best_run_id])\r\n",
    "print(\"Metrics of best run:\\n\", metrics[best_run_id])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "# Step 2 bis (choice 2 or 2 bis) Run AutomML\r\n",
    "automl_settings = {\r\n",
    "    \"iteration_timeout_minutes\" : 10,\r\n",
    "    \"iterations\" : 4,\r\n",
    "    \"experiment_timeout_hours\" : 0.10,\r\n",
    "    \"primary_metric\" : 'Precision'\r\n",
    "}\r\n",
    "\r\n",
    "aml_run_config = RunConfiguration()\r\n",
    "automl_config = AutoMLConfig(task = 'classification',\r\n",
    "                             path = '.',\r\n",
    "                             debug_log = 'automated_ml_errors.log',\r\n",
    "                             compute_target = 'jcharley2',\r\n",
    "                             run_configuration = aml_run_config,\r\n",
    "                             featurization = 'auto',\r\n",
    "                             training_data = [prepared_ds.read_delimited_files().as_input(name='prepared_ds')],\r\n",
    "                             label_column_name = 'EmployeeTargeted',\r\n",
    "                             **automl_settings)\r\n",
    "                             \r\n",
    "# add to the pipeline\r\n",
    "step2_bis = AutoMLStep(name='AutoML',\r\n",
    "                automl_config=automl_config,\r\n",
    "                passthru_automl_config=False,\r\n",
    "                #outputs=[metrics_data,model_data],\r\n",
    "                enable_default_model_output=False,\r\n",
    "                enable_default_metrics_output=False,\r\n",
    "                allow_reuse=True)\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "f4d2fc3f9599656570a986cacd0f8fb633e1f5eb6c0d7edd902095b821fab887"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}